{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PCA['tsne-pca50-one'] = tsne_pca_results[:,0]\n",
    "df_PCA['tsne-pca50-two'] = tsne_pca_results[:,1]\n",
    "plt.figure(figsize=(16,4))\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "sns.scatterplot(\n",
    "    x=\"pca-one\", y=\"pca-two\",\n",
    "    hue=\"pca-two\",  # Replace 'y' with the actual column name\n",
    "    palette=sns.color_palette(\"hls\", 10),\n",
    "    data=df_PCA,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3,\n",
    "    ax=ax1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "components = pca.fit_transform(df_PCA)\n",
    "labels = {\n",
    "    str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
    "    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n",
    "}\n",
    "\n",
    "fig = px.scatter_matrix(\n",
    "    components,\n",
    "    labels=labels,\n",
    "    dimensions=range(10),\n",
    "    color=df_PCA[\"class\"]\n",
    ")\n",
    "fig.update_traces(diagonal_visible=False)\n",
    "fig.update_layout(\n",
    "    font=dict(size=10),  # Adjust font size\n",
    "    width=1200,  # Adjust width\n",
    "    height=1200,  # Adjust height\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300)\n",
    "tsne_pca_results = tsne.fit_transform(df_PCA)\n",
    "\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(X_transformed)\n",
    "\n",
    "# Visualize PCA results\n",
    "plt.figure(figsize=(16, 4))\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "sns.scatterplot(\n",
    "    x=\"pc1\", y=\"pc2\",\n",
    "    data=df_PCA,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3,\n",
    "    ax=ax1\n",
    ")\n",
    "ax1.set_title('PCA')\n",
    "\n",
    "# Perform t-SNE on PCA results\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=0, perplexity=40, n_iter=300)\n",
    "tsne_pca_results = tsne.fit_transform(pca_result)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "\n",
    "# Add t-SNE results to DataFrame\n",
    "df_PCA['tsne-pca-one'] = tsne_pca_results[:,0]\n",
    "df_PCA['tsne-pca-two'] = tsne_pca_results[:,1]\n",
    "\n",
    "# Visualize t-SNE on PCA results\n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-pca-one\", y=\"tsne-pca-two\",\n",
    "    data=df_PCA,\n",
    "    legend=\"full\",\n",
    "    alpha=0.3,\n",
    "    ax=ax2\n",
    ")\n",
    "ax2.set_title('t-SNE on PCA')\n",
    "\n",
    "# Finally, display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load faulty data\n",
    "df_block = pd.read_csv('Class2..air blockage.csv')\n",
    "df_leak  = pd.read_csv('Classs 1..air leakage.csv')\n",
    "df_block.head()\n",
    "# drop class column \n",
    "df_block = df_block.drop(['class'], axis=1)\n",
    "block_dataframe = pd.DataFrame(df_block)\n",
    "df_leak = df_leak.drop(['class'], axis=1)\n",
    "leak_dataframe = pd.DataFrame(df_leak)\n",
    "\n",
    "# TODO: scale and PCA on faulty test data and  assign a class =1 and a class=2\n",
    "data_pca_block = pipe.fit_transform(block_dataframe)\n",
    "data_pca_leak = pipe.fit_transform(leak_dataframe)\n",
    "df_pca_block = pd.DataFrame(data_pca_block, columns=['pc1', 'pc2', 'pc3','pc4','pc5','pc6','pc7','pc8','pc9','pc10'])\n",
    "df_pca_block['class']=1\n",
    "df_pca_leak =  pd.DataFrame(data_pca_leak,  columns=['pc1', 'pc2', 'pc3','pc4','pc5','pc6','pc7','pc8','pc9','pc10'])\n",
    "df_pca_leak['class']=2\n",
    "\n",
    "# TODO: combine the normal and faulty data into one dataframe\n",
    "df_pca_combined = pd.concat([df_PCA, df_pca_block,df_pca_leak])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load faulty data\n",
    "df_block = pd.read_csv('Class2..air blockage.csv')\n",
    "df_leak = pd.read_csv('Classs 1..air leakage.csv')\n",
    "\n",
    "# Drop class column \n",
    "df_block = df_block.drop(['class'], axis=1)\n",
    "df_leak = df_leak.drop(['class'], axis=1)\n",
    "\n",
    "# Ensure column names match the fitted pipeline\n",
    "df_block_transformed = pd.DataFrame(pipe.transform(df_block), columns=df_PCA.columns[:-1])\n",
    "df_leak_transformed = pd.DataFrame(pipe.transform(df_leak), columns=df_PCA.columns[:-1])\n",
    "\n",
    "# Assign class labels\n",
    "df_block_transformed['class'] = 1\n",
    "df_leak_transformed['class'] = 2\n",
    "\n",
    "# Combine normal and faulty data into one dataframe\n",
    "df_combined = pd.concat([df_PCA, df_block_transformed, df_leak_transformed], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2--</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_block = pd.read_csv('Class2..air blockage.csv')\n",
    "df_leak  = pd.read_csv('Classs 1..air leakage.csv')\n",
    "\n",
    "# drop class column \n",
    "df_block = df_block.drop(['class'], axis=1)\n",
    "block_dataframe = pd.DataFrame(df_block)\n",
    "df_leak = df_leak.drop(['class'], axis=1)\n",
    "leak_dataframe = pd.DataFrame(df_leak)\n",
    "\n",
    "# Define a new pipeline without the 'Hour of day' feature\n",
    "new_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=10))\n",
    "])\n",
    "\n",
    "# Drop the 'Hour of day' feature from the original DataFrame before fitting the new pipeline\n",
    "df_no_hour_of_day = df.drop('Hour of day', axis=1)\n",
    "\n",
    "# Fit the new pipeline on the modified DataFrame\n",
    "new_pipe.fit(df_no_hour_of_day)\n",
    "\n",
    "# Transform test data using the new pipeline\n",
    "data_pca_block = new_pipe.transform(df_block)\n",
    "data_pca_leak = new_pipe.transform(df_leak)\n",
    "\n",
    "# Create DataFrame with transformed data\n",
    "df_pca_block = pd.DataFrame(data_pca_block, columns=['pc1', 'pc2', 'pc3', 'pc4', 'pc5', 'pc6', 'pc7', 'pc8', 'pc9', 'pc10'])\n",
    "df_pca_block['class'] = 1\n",
    "\n",
    "df_pca_leak = pd.DataFrame(data_pca_leak, columns=['pc1', 'pc2', 'pc3', 'pc4', 'pc5', 'pc6', 'pc7', 'pc8', 'pc9', 'pc10'])\n",
    "df_pca_leak['class'] = 2\n",
    "\n",
    "# Combine the normal and faulty data into one dataframe\n",
    "df_pca_combined = pd.concat([df_PCA, df_pca_block, df_pca_leak])\n",
    "df_pca_combined.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate anomaly scores for the combined data\n",
    "df_pca_combined['score'] = gmm.score_samples(df_pca_combined[['pc1', 'pc2', 'pc3', 'pc4', 'pc5', 'pc6', 'pc7', 'pc8', 'pc9', 'pc10']])\n",
    "\n",
    "\n",
    "# Visualize the scores using a histogram\n",
    "def plot_scores_histogram(scores, percentile=50):\n",
    "    \"\"\"\n",
    "    Plot a histogram of anomaly scores and mark the threshold\n",
    "    \"\"\"\n",
    "    plt.hist(scores, bins=5)\n",
    "    threshold = np.percentile(scores, percentile)\n",
    "    plt.axvline(threshold, color='red', linestyle='dashed', linewidth=2, label=f'{percentile}% threshold')\n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Scores')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot the histogram\n",
    "plot_scores_histogram(df_pca_combined['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>4</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 1. calculate the anomaly scores for the combined data\n",
    "# detect anomalies using GMM\n",
    "# 1. calculate the score (log probability) of each sample\n",
    "df_pca_combined['score'] = gmm.score_samples(df_pca_combined[['pc1', 'pc2', 'pc3', 'pc4', 'pc5', 'pc6', 'pc7', 'pc8', 'pc9', 'pc10']])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: visualize the scores using a histogram\n",
    "\n",
    "# visualize scores using a histogram\n",
    "plt.hist(df_pca_combined['score'], bins=20)\n",
    "plt.axvline(np.percentile(df_pca_combined['score'], 100), color='red', linestyle='dashed', linewidth=2)\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>5</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate anomaly scores for the combined data\n",
    "df_pca_combined['score'] = gmm.score_samples(df_pca_combined[['pc1', 'pc2', 'pc3', 'pc4', 'pc5', 'pc6', 'pc7', 'pc8', 'pc9', 'pc10']])\n",
    "\n",
    "def plot_scores_histogram(scores, percentile=100):\n",
    "    \"\"\"\n",
    "    Plot a histogram of anomaly scores and mark the threshold\n",
    "    \"\"\"\n",
    "    plt.hist(scores, bins=5, edgecolor='black', linewidth=1.2)\n",
    "    threshold = np.percentile(scores, percentile)\n",
    "    plt.axvline(threshold, color='red', linestyle='dashed', linewidth=2, label=f'{percentile}% threshold')\n",
    "    plt.xlabel('Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Scores')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_scores_histogram(df_pca_combined['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>6</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score\n",
    "\n",
    "# Assuming you have a ground truth label column in your dataframe called 'true_label'\n",
    "y_true = df_PCA.drop('group') # Replace 'true_label' with the actual name of your ground truth label column\n",
    "y_pred = df_PCA['anomaly']\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Classification Report (includes Precision, Recall, and F1-Score)\n",
    "class_report = classification_report(y_true, y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "# ROC-AUC Score\n",
    "# Note: ROC-AUC can only be used if 'anomaly' is a binary classification\n",
    "roc_auc = roc_auc_score(y_true, y_pred)\n",
    "print(\"ROC-AUC Score:\", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>7</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_PCA['group'], df_pca_combined['anomaly']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>8</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'group' is the column with your group labels\n",
    "# and 'anomaly_score' is the column with the anomaly scores from Isolation Forest\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_PCA.index, df_PCA['anomaly_score'], c=df_PCA['group'], cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(label='Group')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Anomaly Score')\n",
    "plt.title('Anomaly Score Distribution by Group')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>9</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to create a heatmap for a pair of principal components\n",
    "def create_heatmap(df, pc_x, pc_y, group_column):\n",
    "    # Pivot the dataframe to create a matrix suitable for a heatmap\n",
    "    heatmap_data = df.pivot_table(index=pc_x, columns=pc_y, values=group_column, aggfunc=lambda x: x)\n",
    "    \n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt='')\n",
    "    \n",
    "    # Add labels and a title\n",
    "    plt.xlabel(pc_y)\n",
    "    plt.ylabel(pc_x)\n",
    "    plt.title(f'Heatmap of Anomaly Groups for {pc_x} vs {pc_y}')\n",
    "    \n",
    "    # Display the heatmap\n",
    "    plt.show()\n",
    "\n",
    "# Assuming 'group' is a categorical variable with values 'Group 1', 'Group 2', 'Group 3'\n",
    "# Convert 'group' to a categorical type with ordered categories\n",
    "df_PCA['group'] = pd.Categorical(df_PCA['group'], categories=['Group 1', 'Group 2', 'Group 3'], ordered=True)\n",
    "\n",
    "# Loop through the principal components in pairs and create a heatmap for each pair\n",
    "for i in range(1, 10, 2):  # Adjust the range as needed\n",
    "    pc_x = f'pc{i}'\n",
    "    pc_y = f'pc{i+1}'\n",
    "    create_heatmap(df_PCA, pc_x, pc_y, 'group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>10</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert 'group' to numerical values\n",
    "group_mapping = {'Group 1': 1, 'Group 2': 2, 'Group 3': 3}\n",
    "df_PCA['group_num'] = df_PCA['group'].map(group_mapping)\n",
    "\n",
    "# Function to create a heatmap for a pair of principal components\n",
    "def create_heatmap(df, pc_x, pc_y, group_column):\n",
    "    # Pivot the dataframe to create a matrix suitable for a heatmap\n",
    "    # Use 'group_num' instead of 'group' to get numerical values\n",
    "    heatmap_data = df.pivot_table(index=pc_x, columns=pc_y, values=group_column, aggfunc=np.mean)\n",
    "    \n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(heatmap_data, annot=True, fmt='.1f')\n",
    "    \n",
    "    # Add labels and a title\n",
    "    plt.xlabel(pc_y)\n",
    "    plt.ylabel(pc_x)\n",
    "    plt.title(f'Heatmap of Anomaly Groups for {pc_x} vs {pc_y}')\n",
    "    \n",
    "    # Display the heatmap\n",
    "    plt.show()\n",
    "\n",
    "# Loop through the principal components in pairs and create a heatmap for each pair\n",
    "for i in range(1, 10, 2):  # Adjust the range as needed\n",
    "    pc_x = f'pc{i}'\n",
    "    pc_y = f'pc{i+1}'\n",
    "    create_heatmap(df_PCA, pc_x, pc_y, 'group_num')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very Impo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Gaussian Mixture Model to the anomalies\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Assuming your data is stored in the variable X\n",
    "# X should be a numpy array or pandas DataFrame\n",
    "\n",
    "# Fit the GMM model\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "gmm.fit(df_PCA)\n",
    "\n",
    "# Get the predicted labels for each data point\n",
    "labels = gmm.predict(df_PCA)\n",
    "\n",
    "# Find the cluster with the highest probability for each data point\n",
    "probs = gmm.predict_proba(df_PCA)\n",
    "max_probs = np.max(probs, axis=1)\n",
    "\n",
    "# Define a threshold for classifying anomalies\n",
    "threshold = 0.9999 # Adjust this threshold as needed\n",
    "\n",
    "# Separate the data into normal and anomaly clusters\n",
    "normal_data = X[max_probs >= threshold]\n",
    "anomaly_data = X[max_probs < threshold]\n",
    "\n",
    "gmm = GaussianMixture(n_components=2, random_state=RANDOM_SEED)\n",
    "gmm.fit(anomaly_data)\n",
    "\n",
    "# Predict the cluster labels for the anomalies\n",
    "anomaly_labels = gmm.predict(anomaly_data)\n",
    "\n",
    "# Add the cluster labels to the anomaly_data DataFrame\n",
    "anomaly_data['Cluster'] = anomaly_labels\n",
    "\n",
    "# Separate the anomalies into two clusters\n",
    "cluster_0 = anomaly_data[anomaly_data['Cluster'] == 0]\n",
    "cluster_1 = anomaly_data[anomaly_data['Cluster'] == 1]\n",
    "\n",
    "# Print the number of data points in each cluster\n",
    "print(\"Number of data points in normal cluster:\", len(normal_data))\n",
    "print(\"Number of data points in anomaly cluster:\", len(anomaly_data))\n",
    "\n",
    "anomaly_data.to_csv('anomaly_data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imp too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Gaussian Mixture Model to the anomalies\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Assuming your data is stored in the variable X\n",
    "# X should be a numpy array or pandas DataFrame\n",
    "\n",
    "# Fit the GMM model\n",
    "gmm = GaussianMixture(n_components=3, random_state=42)\n",
    "gmm.fit(df_PCA)\n",
    "\n",
    "# Get the predicted labels for each data point\n",
    "labels = gmm.predict(df_PCA)\n",
    "\n",
    "# Find the cluster with the highest probability for each data point\n",
    "probs = gmm.predict_proba(df_PCA)\n",
    "max_probs = np.max(probs, axis=1)\n",
    "\n",
    "# Define a threshold for classifying anomalies\n",
    "threshold = 0.9999 # Adjust this threshold as needed\n",
    "\n",
    "# Separate the data into normal and anomaly clusters\n",
    "normal_data = X[max_probs >= threshold]\n",
    "anomaly_data = X[max_probs < threshold]\n",
    "\n",
    "gmm = GaussianMixture(n_components=2, random_state=RANDOM_SEED)\n",
    "gmm.fit(anomaly_data)\n",
    "\n",
    "# Predict the cluster labels for the anomalies\n",
    "anomaly_labels = gmm.predict(anomaly_data)\n",
    "\n",
    "# Add the cluster labels to the anomaly_data DataFrame\n",
    "anomaly_data['Cluster'] = anomaly_labels\n",
    "\n",
    "# Separate the anomalies into two clusters\n",
    "cluster_0 = anomaly_data[anomaly_data['Cluster'] == 0]\n",
    "cluster_1 = anomaly_data[anomaly_data['Cluster'] == 1]\n",
    "# Number of normal data\n",
    "num_normal_data = len(X_transformed)\n",
    "print(\"Number of normal data:\", num_normal_data)\n",
    "\n",
    "# Number of anomaly data in cluster 1\n",
    "num_anomaly_cluster_1 = len(anomaly_data[anomaly_data['Cluster'] == 0])\n",
    "print(\"Number of anomaly data in cluster 1:\", num_anomaly_cluster_1)\n",
    "\n",
    "# Number of anomaly data in cluster 2\n",
    "num_anomaly_cluster_2 = len(anomaly_data[anomaly_data['Cluster'] == 1])\n",
    "print(\"Number of anomaly data in cluster 2:\", num_anomaly_cluster_2)\n",
    "\n",
    "anomaly_data.to_csv('anomaly_data.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
